{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from resnets_utils import *\n",
    "\n",
    "import imageio\n",
    "from keras.models import load_model\n",
    "from sklearn.datasets import load_files   \n",
    "from keras.utils import np_utils\n",
    "from glob import glob\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras import optimizers\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
    "from keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory:        D:\\deep_udacity\\deep-learning\\kaggle\\widsdatathon2019\n",
      "Training images path:  .\\train_images \n",
      "Test images path:      .\\leaderboard_test_data \n",
      "Holdout images path:   .\\leaderboard_holdout_data \n",
      "Sample submission:     .\\SampleSubmission.csv \n",
      "Training Labels path:  .\\traininglabels.csv \n"
     ]
    }
   ],
   "source": [
    "#Initialize paths related to training and testing datasets\n",
    "print(\"Base directory:        %s\" % os.getcwd())\n",
    "\n",
    "path =  '.'\n",
    "train_folder = os.path.join(path, 'train_images')\n",
    "test_folder =  os.path.join(path,  'leaderboard_test_data')\n",
    "holdout_folder = os.path.join(path,  'leaderboard_holdout_data')\n",
    "sample_sub = os.path.join(path, 'SampleSubmission.csv')\n",
    "labels = os.path.join(path,  'traininglabels.csv')\n",
    "\n",
    "print(\"Training images path:  %s \" % train_folder)\n",
    "print(\"Test images path:      %s \" % test_folder)\n",
    "print(\"Holdout images path:   %s \" % holdout_folder)\n",
    "print(\"Sample submission:     %s \" % sample_sub)\n",
    "print(\"Training Labels path:  %s \" % labels)\n",
    "image_size=256\n",
    "image_layers=3\n",
    "pixel_depth=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to load images available in a folder\n",
    "def load_images_from_folder(folder_name):\n",
    "    \n",
    "    test_image_names = []\n",
    "    image_files = os.listdir(folder_name)\n",
    "    loaded_images = np.ndarray(shape=(len(image_files), image_size, image_size,image_layers), dtype=np.float32)\n",
    "\n",
    "    num_images = 0\n",
    "\n",
    "    for image_name in image_files:\n",
    " \n",
    "        image_file = os.path.join(folder_name, image_name)\n",
    "        try:\n",
    "          image_data = (imageio.imread(image_file).astype(float) ) / pixel_depth\n",
    "          loaded_images[num_images, :, :,:] = image_data\n",
    "          num_images = num_images + 1\n",
    "          test_image_names.append(image_name)\n",
    "        \n",
    "        except (IOError, ValueError) as e:\n",
    "          print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    #If we could not read any image, resize the array to correct length\n",
    "    loaded_images = loaded_images[0:num_images, :, :,:]\n",
    " \n",
    "    return test_image_names, loaded_images\n",
    "\n",
    "test_image_names, test_images = load_images_from_folder( os.path.join(path,  'leaderboard_holdout_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size  (2178, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing size \" , str(test_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "loaded_model.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178/2178 [==============================] - ETA: 23:1 - ETA: 23:4 - ETA: 24:0 - ETA: 24:1 - ETA: 24:3 - ETA: 24:4 - ETA: 24:5 - ETA: 25:0 - ETA: 25:5 - ETA: 26:4 - ETA: 27:2 - ETA: 27:4 - ETA: 27:2 - ETA: 27:2 - ETA: 27:3 - ETA: 27:1 - ETA: 27:0 - ETA: 27:0 - ETA: 26:5 - ETA: 26:3 - ETA: 26:2 - ETA: 26:0 - ETA: 25:4 - ETA: 25:3 - ETA: 25:1 - ETA: 24:5 - ETA: 24:4 - ETA: 24:2 - ETA: 24:1 - ETA: 23:5 - ETA: 23:4 - ETA: 23:2 - ETA: 23:1 - ETA: 22:5 - ETA: 22:4 - ETA: 22:2 - ETA: 22:1 - ETA: 22:0 - ETA: 21:4 - ETA: 21:3 - ETA: 21:2 - ETA: 21:0 - ETA: 20:5 - ETA: 20:4 - ETA: 20:2 - ETA: 20:1 - ETA: 20:0 - ETA: 19:4 - ETA: 19:3 - ETA: 19:2 - ETA: 19:0 - ETA: 18:5 - ETA: 18:3 - ETA: 18:2 - ETA: 18:1 - ETA: 18:0 - ETA: 17:5 - ETA: 17:4 - ETA: 17:3 - ETA: 17:1 - ETA: 17:0 - ETA: 16:5 - ETA: 16:4 - ETA: 16:2 - ETA: 16:1 - ETA: 16:0 - ETA: 15:4 - ETA: 15:3 - ETA: 15:2 - ETA: 15:1 - ETA: 14:5 - ETA: 14:4 - ETA: 14:3 - ETA: 14:2 - ETA: 14:0 - ETA: 13:5 - ETA: 13:4 - ETA: 13:3 - ETA: 13:1 - ETA: 13:0 - ETA: 12:5 - ETA: 12:3 - ETA: 12:2 - ETA: 12:0 - ETA: 11:5 - ETA: 11:3 - ETA: 11:2 - ETA: 11:1 - ETA: 10:5 - ETA: 10:4 - ETA: 10:3 - ETA: 10:2 - ETA: 10:0 - ETA: 9:5 - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 58s - ETA: 44 - ETA: 30 - ETA: 15 - ETA: 1 - 1929s 886ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict(test_images,  batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing size  (2178, 256, 256, 3)\n",
      "Predictions array  (2178, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing size \" , str(test_images.shape))\n",
    "print(\"Predictions array \" , str(preds.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True,   formatter={'float_kind':'{:0.0f}'.format}) \n",
    "# print (preds[:,1])\n",
    "# print(test_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2178"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "file_names = OrderedDict([ ('image_id', test_image_names), ('has_oilpalm', np.around(preds[:,1], decimals=0).astype(int)) ])\n",
    " \n",
    "df = pd.DataFrame.from_dict(file_names)\n",
    "df.to_csv(\"leaderboard_holdout_data.csv\", index = False, sep=',', encoding='utf-8')\n",
    "\n",
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py3]",
   "language": "python",
   "name": "conda-env-ipykernel_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
